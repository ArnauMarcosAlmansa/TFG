With activation functions like ReLU, a network cannot learn information of the derivative of a function. With tanh, softplus, and specially cos and sin, it can.  
Setting the weights to 6 may be better than initializing them randomly. Also setting the bias to 30.
Use ADAM optimizer.
SIRENs can be used to store images and reconstruct them. They also can be used to inpaint images of which we don't know the entirety of the pixels.
[Source](https://arxiv.org/pdf/2006.09661.pdf).
